{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ef299f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/cohlem/Projects/Experimentation/RL-GRPO'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d05d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.testing._internal.common_distributed import spawn_threads_and_init_comms\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import os\n",
    "from torch import distributed as dist\n",
    "import torch.nn as nn\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "# from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy\n",
    "from torch.distributed._composable.fsdp import fully_shard\n",
    "from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel, RowwiseParallel, SequenceParallel, PrepareModuleInput\n",
    "from torch.distributed._tensor import Shard, Replicate, distribute_tensor, DTensor\n",
    "\n",
    "# from llama2_model import Transformer, ModelArgs\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch\n",
    "from torch.distributed._tensor import DeviceMesh, Shard, Replicate, distribute_tensor\n",
    "\n",
    "\n",
    "from torch.distributed._composable.fsdp import fully_shard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05067fab",
   "metadata": {},
   "source": [
    "# Let's fucking goooo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f211bb0",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "\n",
    " - load data from huggingface hub using hf datasets\n",
    " - make a dataloader (shuffle data, set batch sizes and so on)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d7b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset('CohleM/rlpr_test_small', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f923efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab3db8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "840ce3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.data = load_dataset(path, split='train')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.data['prompt'][idx]\n",
    "        answer = self.data['reward_model'][idx]\n",
    "        return {'question' : question , 'answer' : answer }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13e0cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset('CohleM/rlpr_test_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "875432b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea3531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10eae48d",
   "metadata": {},
   "source": [
    "### training algorithm\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "mesh = init_device_mesh(3,2) # 3 DDP, 2 TP\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(the_dataset, shuffle=True, batch_size=64)\n",
    "for train_data in train_dataloader: # each train_data is of len batch_size=64\n",
    "    # split the data into different 3 DDP groups\n",
    "\n",
    "    complete_rollout = []\n",
    "    step = 0\n",
    "    \n",
    "    \n",
    "    for each_data in train_data:\n",
    "        each_data = each_data.reapeat(rollout.n)\n",
    "        \n",
    "        # generate responses.\n",
    "        responses = llm.generate(each_data)\n",
    "        advantages = calculate_advantages(each_data, responses)\n",
    "        complete_rollout.append(each_data, responses)\n",
    "    \n",
    "    old_llm = make_copy_of_current_llm()    \n",
    "    for update in range(no_of_updates_per_ppo_step):\n",
    "        minibatches = DataLoader(complete_rollout, batch_size=ppo_mini_batch_size) # no shuffle here\n",
    "        for mini_batch in minibatches:\n",
    "            # generate the logprobs \n",
    "            old_logprobs = old_llm(mini_batchl)\n",
    "            logprobs = llm(mini_batch)\n",
    "            ppo_loss = calculate_ppo_loss(old_logprobs, logprobs, advantages) # i.e ratio * adv, where ratio = logprobs/ old_logprobs\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "         \n",
    "            \n",
    "        \n",
    "        \n",
    "    # we're doing GRPO so for each question generate,  \n",
    "    \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c5d4d",
   "metadata": {},
   "source": [
    "### training algorithm\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "mesh = init_device_mesh(3,2) # 3 DDP, 2 TP\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(the_dataset, shuffle=True, batch_size=64)\n",
    "for train_data in train_dataloader: # each train_data is of len batch_size=64\n",
    "    # split the data into different 3 DDP groups\n",
    "\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    \n",
    "    # repeat the train_data X rollout.n = new_train_data\n",
    "    # split the new_train_data into different 3 DDP groups processes, each will get (len(new_train_data) / DDP_size) data\n",
    "    new_train_data = train_data.repeat(rollout.n)\n",
    "        \n",
    "    #split here, i.e new_train_data into 3 ddp groups,\n",
    "    # we also need an estimate of how many data each group can process at a time, lets say it's vllm_mini_batch\n",
    "    new_train_data = DataLoader(new_train_data, vllm_mini_batch)\n",
    "    complete_rollout = []\n",
    "    for vllm_batch in new_train_data:\n",
    "        # generate responses.\n",
    "        responses = llm.generate(vllm_batch)\n",
    "        complete_rollout.append(vllm_batch,responses)\n",
    "    \n",
    "     \n",
    "    \n",
    "    # all_gather data into one process then caculate the return/advantages,\n",
    "    # after gathering complete_rollout should be full.\n",
    "    \n",
    "    advantages = calculate_advantages(each_data, responses)\n",
    "    complete_rollout.append(each_data, responses)\n",
    "    \n",
    "    \n",
    "    old_llm = make_copy_of_current_llm()  # we only need model parameters for old_llm, and will have the same sharding strategy as the current llm i.e llm.\n",
    "    \n",
    "    for update in range(no_of_updates_per_ppo_step):\n",
    "        minibatches = DataLoader(complete_rollout, batch_size=ppo_mini_batch_size) # no shuffle here, or maybe split the mini_batches \n",
    "        for mini_batch in minibatches:\n",
    "            # generate the logprobs \n",
    "            old_logprobs = old_llm(mini_batch)\n",
    "            logprobs = llm(mini_batch)\n",
    "            ppo_loss = calculate_ppo_loss(old_logprobs, logprobs, advantages) # i.e ratio * adv, where ratio = logprobs/ old_logprobs\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "         \n",
    "            \n",
    "     # update the vllm model with our llm's weight.\n",
    "    \n",
    "        \n",
    "    # we're doing GRPO so for each question generate,  \n",
    "    \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2f3ea5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(50).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19187d",
   "metadata": {},
   "source": [
    "# PPO trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46e0f4",
   "metadata": {},
   "source": [
    "dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81bb9c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    train_batch_size: int = 64\n",
    "    rollout_n: int = 8\n",
    "    mini_batch_size: int = 8\n",
    "    model_name: str = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "    ddp_size: int = 2\n",
    "    tp_size: int = 2\n",
    "    lr: float = 1e-6\n",
    "#     train_batch_size: int = 64\n",
    "        \n",
    "config = Config()\n",
    "\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.distributed._composable.fsdp import fully_shard\n",
    "from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel, RowwiseParallel, SequenceParallel, PrepareModuleInput\n",
    "from llama2_model import Transformer, ModelArgs\n",
    "\n",
    "class Worker:\n",
    "    \"\"\"\n",
    "    This is the policy that we will be updating with each gradient update, we rollout using this policy's\n",
    "    parameters, and we use the logprobs from this policy, we will also copy it's weights to make it old policy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        self.config = config\n",
    "        device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # define model from huggingface later on\n",
    "        simple_llama2_config = ModelArgs(dim=4, n_layers=1, n_heads=4, vocab_size=8)\n",
    "        self.model = Transformer.from_model_args(simple_llama2_config).to(device)\n",
    "\n",
    "        # first make a device mesh\n",
    "        fsdp_size = int(int(os.environ['WORLD_SIZE']) / (config.ddp_size * config.tp_size))\n",
    "        self.mesh = init_device_mesh(device,(config.ddp_size,fsdp_size, config.tp_size), mesh_dim_names=[\"DDP\", \"FSDP\", \"TP\"])\n",
    "\n",
    "#         # shard model's parameters on tp axis\n",
    "#         self.prepare_tp_model()\n",
    "\n",
    "#         # shard model's parameter on dp axis\n",
    "#         prepare_dp_model()\n",
    "\n",
    "    def prepare_optimizer(self):\n",
    "        if config.tp_size > 1:\n",
    "            self.model = prepare_tp_model(self.model, self.mesh)\n",
    "        \n",
    "        self.model = prepare_dp_model(self.model, self.mesh)\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.config.lr)\n",
    "#         if dist.get_rank() == 0:\n",
    "#             print(f' after dp model rank: {dist.get_rank()} attention wq {self.model.layers[0].attention.wq.weight}')\n",
    "        \n",
    "        \n",
    "def prepare_tp_model(model, mesh):\n",
    "\n",
    "    layer_tp_plan = {\n",
    "        # Now the input and output of SequenceParallel has Shard(1) layouts,\n",
    "        # to represent the input/output tensors sharded on the sequence dimension\n",
    "        ### ------ The module names will be different for new models -----, take a look at model.named_parameters()\n",
    "        \"attention_norm\": SequenceParallel(),\n",
    "        \"attention\": PrepareModuleInput(\n",
    "            input_layouts=(Shard(1), Replicate()),\n",
    "            desired_input_layouts=(Replicate(), Replicate()),\n",
    "        ),\n",
    "        \"attention.wq\": ColwiseParallel(use_local_output=False),\n",
    "        \"attention.wk\": ColwiseParallel(use_local_output=False),\n",
    "        \"attention.wv\": ColwiseParallel(use_local_output=False),\n",
    "        \"attention.wo\": RowwiseParallel(output_layouts=Shard(1)),\n",
    "        \"ffn_norm\": SequenceParallel(),\n",
    "        \"feed_forward\": PrepareModuleInput(\n",
    "            input_layouts=(Shard(1),),\n",
    "            desired_input_layouts=(Replicate(),),\n",
    "        ),\n",
    "        \"feed_forward.w1\": ColwiseParallel(),\n",
    "        \"feed_forward.w2\": RowwiseParallel(output_layouts=Shard(1)),\n",
    "        \"feed_forward.w3\": ColwiseParallel(),\n",
    "    }\n",
    "\n",
    "\n",
    "    # Apply TP\n",
    "    for layer_id, transformer_block in enumerate(model.layers):\n",
    "\n",
    "        parallelize_module(\n",
    "            module=transformer_block,\n",
    "            device_mesh=mesh['TP'],\n",
    "            parallelize_plan=layer_tp_plan,\n",
    "        )\n",
    "\n",
    "    model = parallelize_module(\n",
    "        model,\n",
    "        mesh['TP'],\n",
    "        {\n",
    "            \"tok_embeddings\": RowwiseParallel(\n",
    "                input_layouts=Replicate(),\n",
    "                output_layouts=Shard(1),\n",
    "            ),\n",
    "            \"norm\": SequenceParallel(),\n",
    "            \"output\": ColwiseParallel(\n",
    "                input_layouts=Shard(1),\n",
    "                output_layouts=Replicate()\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def prepare_dp_model(model, mesh):\n",
    "    for layer in model.layers:\n",
    "        fully_shard(layer, mesh=mesh['DDP','FSDP'])\n",
    "\n",
    "    sharded_model = fully_shard(model, mesh=mesh['DDP','FSDP'])\n",
    "    return sharded_model\n",
    "\n",
    "\n",
    "class Actor(Worker):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "#         define model from huggingface later on\n",
    "        simple_llama2_config = ModelArgs(dim=4, n_layers=1, n_heads=4, vocab_size=8)\n",
    "        self.model = Transformer.from_model_args(simple_llama2_config).to(device)\n",
    "        \n",
    "        # actor will need optimizer\n",
    "        self.prepare_optimizer()\n",
    "\n",
    "\n",
    "class Rollout(Worker):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "            \n",
    "        # init model using sglang\n",
    "        self.prepare_env_var()\n",
    "\n",
    "        if self.mesh[\"TP\"].get_local_rank() == 0:\n",
    "            os.environ[\"SGLANG_BLOCK_NONZERO_RANK_CHILDREN\"] = \"0\"\n",
    "            engine = Engine(\n",
    "                    model_path=self.config.model_name,\n",
    "                    dtype=\"bfloat16\",\n",
    "                    tp_size=self.mesh[\"TP\"].size(),\n",
    "                    mem_fraction_static=0.5,\n",
    "                    # enable_memory_saver=True,\n",
    "                    port=30000 + dist.get_rank()\n",
    "                )\n",
    "                \n",
    "                \n",
    "    def prepare_env_var(self):\n",
    "        if \"TORCHELASTIC_USE_AGENT_STORE\" in os.environ.keys(): # remove the use of common store for communication\n",
    "            del os.environ[\"TORCHELASTIC_USE_AGENT_STORE\"]\n",
    "        monkey_patch_torch_reductions()\n",
    "        \n",
    "        \n",
    "        # THE reason for doing this is because, we'll store rollout worker's (sglang) weight in these TP group\n",
    "        # otherwise, SGL will use all the available cuda devices.\n",
    "        cuda_visible_devices = [None] * self.config.tp_size\n",
    "        dist.all_gather_object(cuda_visible_devices, os.environ['LOCAL_RANK'], self.mesh['TP'].get_group())\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = \",\".join(cuda_visible_devices)\n",
    "\n",
    "        \n",
    "  \n",
    "class Trainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.actor = Actor(config)\n",
    "#         self.rollout = Rollout(config)\n",
    "        \n",
    "        \n",
    "    def train():\n",
    "        # construct train dataloader\n",
    "        train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=self.config.train_batch_size)\n",
    "        for train_batch in train_dataloader:\n",
    "            # generate rollouts.\n",
    "            # for now use the model.generate() from huggingface\n",
    "            \n",
    "        \n",
    "import os\n",
    "from torch import optim\n",
    "from torch.distributed.checkpoint.state_dict import (\n",
    "    StateDictOptions, get_model_state_dict, get_state_dict\n",
    ")\n",
    "\n",
    "@spawn_threads_and_init_comms\n",
    "def shard_big_tensor(world_size):\n",
    "    os.environ['WORLD_SIZE'] = str(world_size)\n",
    "    os.environ['LOCAL_RANK'] = str(dist.get_rank())\n",
    "    \n",
    "    # can't use os.environ['LOCAL_RANK'] in spawn_threads_and_init_comms so using dist.get_rank() which gives the\n",
    "    # local rank, but instead we must to os.environ['LOCAL_RANK'] otherwise.\n",
    "    \n",
    "    local_rank = dist.get_rank()\n",
    "#     local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    \n",
    "    # if gpu set this\n",
    "    # torch.cuda.set_device(local_rank)\n",
    "    \n",
    "    ppo_trainer = Trainer(config)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39787984",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = 4\n",
    "shard_big_tensor(world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad34f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "58d92579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tokens': [1, 4]}, {'tokens': [1, 4]}, {'tokens': [1, 4]}, {'tokens': [1, 4]}, {'tokens': [1, 4]}, {'tokens': [1, 4]}, {'tokens': [1, 4]}, {'tokens': [1, 4]}, {'tokens': [1, 3]}, {'tokens': [1, 3]}, {'tokens': [1, 3]}, {'tokens': [1, 3]}, {'tokens': [1, 3]}, {'tokens': [1, 3]}, {'tokens': [1, 3]}, {'tokens': [1, 3]}]\n",
      "[{'tokens': [1, 5]}, {'tokens': [1, 5]}, {'tokens': [1, 5]}, {'tokens': [1, 5]}, {'tokens': [1, 5]}, {'tokens': [1, 5]}, {'tokens': [1, 5]}, {'tokens': [1, 5]}]\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.data = [\n",
    "            {\"tokens\": [1, 4]},\n",
    "            {\"tokens\": [1, 3]},\n",
    "            {\"tokens\": [1,5]}\n",
    "        ]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "dataset = MyDataset()\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5dfcfccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6bf9215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return [copy.deepcopy(ex) for ex in batch for _ in range(8)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05941f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9543be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# from RL2.datasets.base import BaseDataset, load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "class RLDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, responses_per_prompt):\n",
    "\n",
    "        self.dataset = load_dataset(data_path, split='train')\n",
    "        self.responses_per_prompt = responses_per_prompt\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        ex = self.dataset[idx]\n",
    "        messages = ex[\"messages\"]\n",
    "        answer = ex[\"answer\"]\n",
    "\n",
    "        return {\n",
    "            \"messages\": messages,\n",
    "            \"answer\": answer\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    responses_per_prompt = 4\n",
    "    return [\n",
    "        copy.deepcopy(ex)\n",
    "        for ex in batch\n",
    "        for _ in range(responses_per_prompt)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "07f7466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = RLDataset('CohleM/olympiad_small', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0f1af6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.stateful_dataloader import StatefulDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7e7ec9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = StatefulDataLoader(test_dataset, batch_size = 3, drop_last=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45c8464",
   "metadata": {},
   "source": [
    "### now for each entires in the data, apply chat template, generate output, generate response_mask, generate reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "33120c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(tokenizer, data_list):\n",
    "    \n",
    "    for data in data_list:\n",
    "        messages,answer = data['messages'], data['answer']\n",
    "        ans = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        print(ans)\n",
    "        \n",
    "        # now generate it through sglang, one benefit of sglang is that we don't don't \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a237d7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Xenia and Sergey play the following game. Xenia thinks of a positive integer $N$ not exceeding 5000. Then she fixes 20 distinct positive integers $a_{1}, a_{2}, \\ldots, a_{20}$ such that, for each $k=1,2, \\ldots, 20$, the numbers $N$ and $a_{k}$ are congruent modulo $k$. By a move, Sergey tells Xenia a set $S$ of positive integers not exceeding 20 , and she tells him back the set $\\left\\{a_{k}: k \\in S\\right\\}$ without spelling out which number corresponds to which index. How many moves does Sergey need to determine for sure the number Xenia thought of?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Xenia and Sergey play the following game. Xenia thinks of a positive integer $N$ not exceeding 5000. Then she fixes 20 distinct positive integers $a_{1}, a_{2}, \\ldots, a_{20}$ such that, for each $k=1,2, \\ldots, 20$, the numbers $N$ and $a_{k}$ are congruent modulo $k$. By a move, Sergey tells Xenia a set $S$ of positive integers not exceeding 20 , and she tells him back the set $\\left\\{a_{k}: k \\in S\\right\\}$ without spelling out which number corresponds to which index. How many moves does Sergey need to determine for sure the number Xenia thought of?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Xenia and Sergey play the following game. Xenia thinks of a positive integer $N$ not exceeding 5000. Then she fixes 20 distinct positive integers $a_{1}, a_{2}, \\ldots, a_{20}$ such that, for each $k=1,2, \\ldots, 20$, the numbers $N$ and $a_{k}$ are congruent modulo $k$. By a move, Sergey tells Xenia a set $S$ of positive integers not exceeding 20 , and she tells him back the set $\\left\\{a_{k}: k \\in S\\right\\}$ without spelling out which number corresponds to which index. How many moves does Sergey need to determine for sure the number Xenia thought of?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Xenia and Sergey play the following game. Xenia thinks of a positive integer $N$ not exceeding 5000. Then she fixes 20 distinct positive integers $a_{1}, a_{2}, \\ldots, a_{20}$ such that, for each $k=1,2, \\ldots, 20$, the numbers $N$ and $a_{k}$ are congruent modulo $k$. By a move, Sergey tells Xenia a set $S$ of positive integers not exceeding 20 , and she tells him back the set $\\left\\{a_{k}: k \\in S\\right\\}$ without spelling out which number corresponds to which index. How many moves does Sergey need to determine for sure the number Xenia thought of?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Given a positive integer $n$, determine the largest real number $\\mu$ satisfying the following condition: for every $4 n$-point configuration $C$ in an open unit square $U$, there exists an open rectangle in $U$, whose sides are parallel to those of $U$, which contains exactly one point of $C$, and has an area greater than or equal to $\\mu$.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Given a positive integer $n$, determine the largest real number $\\mu$ satisfying the following condition: for every $4 n$-point configuration $C$ in an open unit square $U$, there exists an open rectangle in $U$, whose sides are parallel to those of $U$, which contains exactly one point of $C$, and has an area greater than or equal to $\\mu$.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Given a positive integer $n$, determine the largest real number $\\mu$ satisfying the following condition: for every $4 n$-point configuration $C$ in an open unit square $U$, there exists an open rectangle in $U$, whose sides are parallel to those of $U$, which contains exactly one point of $C$, and has an area greater than or equal to $\\mu$.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Given a positive integer $n$, determine the largest real number $\\mu$ satisfying the following condition: for every $4 n$-point configuration $C$ in an open unit square $U$, there exists an open rectangle in $U$, whose sides are parallel to those of $U$, which contains exactly one point of $C$, and has an area greater than or equal to $\\mu$.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "Find (in closed form) the difference between the number of positive integers at most $2^{2017}$ with even weight and the number of positive integers at most $2^{2017}$ with odd weight.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "Find (in closed form) the difference between the number of positive integers at most $2^{2017}$ with even weight and the number of positive integers at most $2^{2017}$ with odd weight.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "Find (in closed form) the difference between the number of positive integers at most $2^{2017}$ with even weight and the number of positive integers at most $2^{2017}$ with odd weight.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "Find (in closed form) the difference between the number of positive integers at most $2^{2017}$ with even weight and the number of positive integers at most $2^{2017}$ with odd weight.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data in data_loader:\n",
    "    print(len(data))\n",
    "#     print(data)\n",
    "    rollout(tokenizer, data)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6768d697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "584b5349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "chunks = [[1, 2], [3, 4], [5]]\n",
    "flattened = sum(chunks, [])\n",
    "print(flattened)  # [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622fa2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a1be79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ea2c224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "501edbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = tokenizer.encode('this is cool', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ddb7b491",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = tokenizer.encode('This is question', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "94093b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "\n",
    "states.extend(question)\n",
    "states.extend(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4d055e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_states = torch.LongTensor(states[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c8ed76b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1986, 374, 3405, 574, 374, 7010]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8e4d7c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_states.shape[-1] - 1) * [0] + [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc566c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075fe757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b80acb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162258b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe6fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e8ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbfd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e63ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6657f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7563c2b",
   "metadata": {},
   "source": [
    "actor changes the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8ae54",
   "metadata": {},
   "source": [
    "# rollout and update works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5cc1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import distributed as dist\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from sglang.srt.entrypoints.engine import Engine\n",
    "import sglang as sgl\n",
    "import nest_asyncio\n",
    "from sglang.test.test_utils import DEFAULT_SMALL_MODEL_NAME_FOR_TEST\n",
    "from transformers import AutoModel\n",
    "from sglang.srt.patch_torch import monkey_patch_torch_reductions\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from torch.distributed._composable.fsdp import fully_shard\n",
    "from torch.distributed._tensor import (DTensor, Replicate, Shard,\n",
    "                                       distribute_tensor)\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.distributed.tensor.parallel import (ColwiseParallel,\n",
    "                                               PrepareModuleInput,\n",
    "                                               RowwiseParallel,\n",
    "                                               SequenceParallel,\n",
    "                                               parallelize_module)\n",
    "\n",
    "\n",
    "from torch.distributed.checkpoint.state_dict import (\n",
    "    StateDictOptions, get_model_state_dict, get_state_dict\n",
    ")\n",
    "\n",
    "from sglang.srt.utils import MultiprocessingSerializer\n",
    "from sglang.srt.model_executor.model_runner import LocalSerializedTensor\n",
    "\n",
    "\n",
    "def setup():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    backend = 'nccl' if device == 'cuda' else 'gloo'\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.set_device(local_rank)\n",
    "    os.environ[\"NCCL_CUMEM_ENABLE\"] = \"0\"\n",
    "    os.environ[\"NCCL_NVLS_ENABLE\"] = \"0\"\n",
    "    # Initialize with explicit parameters\n",
    "    dist.init_process_group(\n",
    "        backend=backend, \n",
    "        world_size=world_size,\n",
    "        rank=rank\n",
    "    )\n",
    "\n",
    "def prepare_llama_tp_layer(layer, device_mesh):\n",
    "\n",
    "    parallelize_plan = {\n",
    "        \"input_layernorm\": SequenceParallel(),\n",
    "        \"self_attn.q_proj\": ColwiseParallel(),\n",
    "        \"self_attn.k_proj\": ColwiseParallel(),\n",
    "        \"self_attn.v_proj\": ColwiseParallel(),\n",
    "        \"self_attn.o_proj\": RowwiseParallel(\n",
    "            output_layouts=Shard(1)\n",
    "        ),\n",
    "        \"post_attention_layernorm\": SequenceParallel(),\n",
    "        \"mlp.gate_proj\": ColwiseParallel(),\n",
    "        \"mlp.up_proj\": ColwiseParallel(),\n",
    "        \"mlp.down_proj\": RowwiseParallel(\n",
    "            output_layouts=Shard(1)\n",
    "        )\n",
    "    }\n",
    "    parallelize_module(\n",
    "        module=layer,\n",
    "        device_mesh=device_mesh,\n",
    "        parallelize_plan=parallelize_plan\n",
    "    )\n",
    "\n",
    "def prepare_environment_variables(mesh):\n",
    "    if \"TORCHELASTIC_USE_AGENT_STORE\" in os.environ.keys():\n",
    "        del os.environ[\"TORCHELASTIC_USE_AGENT_STORE\"]\n",
    "    monkey_patch_torch_reductions()\n",
    "    cuda_visible_devices = mesh[\"TP\"].size() * [None]\n",
    "    dist.all_gather_object(\n",
    "        cuda_visible_devices,\n",
    "        os.environ[\"LOCAL_RANK\"],\n",
    "        mesh[\"TP\"].get_group()\n",
    "    )\n",
    "    # print(f' GLOBAL RNAK {dist.get_rank()} devices {cuda_visible_devices} ')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(cuda_visible_devices)\n",
    "\n",
    "def start():\n",
    "    # nest_asyncio.apply()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    local_rank = dist.get_rank()\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    # torch.cuda.synchronize()\n",
    "    mesh = init_device_mesh(device, (1,2,2), mesh_dim_names = ['DDP','FSDP', 'TP'])\n",
    "\n",
    "    # print(mesh)\n",
    "    # -- rollout ---\n",
    "    prepare_environment_variables(mesh)\n",
    "    # -- rollout ---\n",
    "    \n",
    "    # print(f\"Global rank {dist.get_rank()}, local rank {os.environ['LOCAL_RANK']} mesh {mesh['TP']} visible devices {os.environ[\"CUDA_VISIBLE_DEVICES\"]}\\n\\n  \")\n",
    "    # print(torch.cuda.device_count())\n",
    "\n",
    "    model_name = \"ibm-granite/granite-3.3-2b-base\"\n",
    "\n",
    "    # ------- ROLLOUT ---------\n",
    "    if mesh[\"TP\"].get_local_rank() == 0:\n",
    "        os.environ[\"SGLANG_BLOCK_NONZERO_RANK_CHILDREN\"] = \"0\"\n",
    "        engine = Engine(\n",
    "                model_path=model_name,\n",
    "                dtype=\"bfloat16\",\n",
    "                tp_size=mesh[\"TP\"].size(),\n",
    "                mem_fraction_static=0.5,\n",
    "                # enable_memory_saver=True,\n",
    "                port=30000 + dist.get_rank()\n",
    "            )\n",
    "\n",
    "        # print(engine)\n",
    "        param_name = 'model.layers.0.self_attn.q_proj.weight'\n",
    "        # if dist.get_rank() == 0:\n",
    "        #     breakpoint()\n",
    "\n",
    "        # reserved_gb = torch.cuda.memory_reserved() / (1024 ** 3)\n",
    "        # total_gb = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
    "        # param = engine.get_weights_by_name(param_name)[0][:5]\n",
    "        # print(f\"Global rank {dist.get_rank()}  param : {param} \\n\\n  \")\n",
    "    # ------- ROLLOUT ---------\n",
    "\n",
    "    print(f'gloabal rank {dist.get_rank()} memory allocated {torch.cuda.memory_allocated() }')\n",
    "        \n",
    "    # dist.barrier()\n",
    "\n",
    "    \n",
    "\n",
    "    # ----- rollout -----\n",
    "    # Do the rollout\n",
    "    if mesh[\"TP\"].get_local_rank() == 0:\n",
    "        # release memory temporarily.\n",
    "        engine.release_memory_occupation()\n",
    "        torch.cuda.empty_cache()\n",
    "    # ----- rollout -----\n",
    "\n",
    "    print(f'after release memory oocupation rank {dist.get_rank()} memory allocated {torch.cuda.memory_allocated() }')\n",
    "    \n",
    "    # load the actor model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    # print(model)\n",
    "    print(f'After automodel rank {dist.get_rank()} memory allocated {torch.cuda.memory_allocated() }')\n",
    "    \n",
    "    for layer in model.model.layers:\n",
    "        prepare_llama_tp_layer(layer, mesh['TP'])\n",
    "\n",
    "    \n",
    "# ----- outer block --------\n",
    "    parallelize_plan = {\n",
    "        \"model.embed_tokens\": ColwiseParallel(\n",
    "            output_layouts=Shard(1)\n",
    "        ),\n",
    "        \"model.norm\": SequenceParallel(),\n",
    "        \"lm_head\": ColwiseParallel()\n",
    "    }\n",
    "    parallelize_module(\n",
    "        module=model,\n",
    "        device_mesh=mesh['TP'],\n",
    "        parallelize_plan=parallelize_plan\n",
    "    )\n",
    "# ----- outer block --------\n",
    "\n",
    "# ----------FSDP -----------\n",
    "    for layer in model.model.layers:\n",
    "        fully_shard(layer, mesh=mesh['DDP', 'FSDP'])\n",
    "    \n",
    "    fully_shard(model, mesh=mesh['DDP', 'FSDP'])\n",
    "\n",
    "    # print(f' GLOBAL RANK {dist.get_rank()} {model.model.layers[0].self_attn.q_proj.weight.to_local().shape} memory allocated {torch.cuda.memory_allocated() / (1024 **3) }')\n",
    "\n",
    "\n",
    "\n",
    "    # -- rollout --\n",
    "    # resume memory occupation\n",
    "    torch.cuda.empty_cache()\n",
    "    if mesh[\"TP\"].get_local_rank() == 0:\n",
    "        engine.resume_memory_occupation()\n",
    "\n",
    "\n",
    "    # -- actor changes the model---\n",
    "    # let's make some changes to the first and last row\n",
    "    # -- actor changes the model---\n",
    "    # let's make some changes to the first and last row\n",
    "    new_var = torch.tensor([1.5] * 5).to(torch.cuda.current_device())\n",
    "    # print(model.model.layers[0].self_attn.q_proj.weight)\n",
    "    # print(model.model.layers[0].self_attn.q_proj.weight[0][:5\n",
    "\n",
    "    old_w = model.model.layers[0].self_attn.q_proj.weight\n",
    "    local_weight = old_w.to_local().detach().clone()  \n",
    "    local_weight[0][:5] = new_var\n",
    "\n",
    "    new_mesh = old_w.device_mesh            # reuse original mesh\n",
    "    placements = old_w.placements         # usually (Shard(0),)\n",
    "\n",
    "    new_dt = DTensor.from_local(\n",
    "        local_weight,\n",
    "        device_mesh=new_mesh,\n",
    "        placements=placements,\n",
    "        run_check=False                    # safe because we kept the shape\n",
    "    )\n",
    "\n",
    "    model.model.layers[0].self_attn.q_proj.weight = torch.nn.Parameter(new_dt)\n",
    "\n",
    "    print('after replacement', model.model.layers[0].self_attn.q_proj.weight)\n",
    "\n",
    "\n",
    "    # offload to cpu\n",
    "    options = StateDictOptions(full_state_dict=False, cpu_offload=True)\n",
    "    state_dict = get_model_state_dict(\n",
    "        model, options=options\n",
    "    )\n",
    "    \n",
    "    for idx, (name, tensor) in enumerate(state_dict.items()):\n",
    "        # load to gpu again\n",
    "        tensor = tensor.to(torch.cuda.current_device())\n",
    "        # print(name)\n",
    "        # if name == 'model.layers.0.self_attn.q_proj.weight':\n",
    "            \n",
    "        serialized_tensor = MultiprocessingSerializer.serialize(tensor.full_tensor() if isinstance(tensor, DTensor) else tensor)\n",
    "        serialized_tensors = [None] * mesh['TP'].size() if mesh['TP'].get_local_rank() == 0 else None\n",
    "        \n",
    "        dist.gather_object(serialized_tensor, serialized_tensors, group_dst=0, group=mesh['TP'].get_group())\n",
    "        \n",
    "        if mesh[\"TP\"].get_local_rank() == 0:\n",
    "            # print(serialized_tensors)\n",
    "            engine.update_weights_from_tensor(named_tensors=[(name, LocalSerializedTensor(values=serialized_tensors))])\n",
    "\n",
    "        \n",
    "        # print(f\"rank {dist.get_rank()} seriliazed_tensor {serialized_tensor.shape} len_ST: {len(serialized_tensors) if isinstance(serialized_tensors,list) else serialized_tensors} \")\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "    if mesh[\"TP\"].get_local_rank() == 0:\n",
    "        param_start = engine.get_weights_by_name(param_name)[0][:5]\n",
    "        # param_end = engine.get_weights_by_name(param_name)[-1][:5]\n",
    "        print(f\"Global rank {dist.get_rank()}  param after weight update : {param_start} \\n\\n  \")\n",
    "            \n",
    "            \n",
    "\n",
    "    return\n",
    "\n",
    "        \n",
    "def main():\n",
    "    setup()\n",
    "    start()\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3126e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d3b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531aacef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0a1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06931d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa83f8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c040e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e9a73e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2571bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cf3656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a3e8347",
   "metadata": {},
   "source": [
    "# NEW TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74de293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from torch import distributed as dist\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "import copy\n",
    "from torch.utils.data import Dataset\n",
    "from torchdata.stateful_dataloader import StatefulDataLoader\n",
    "from torch.testing._internal.common_distributed import spawn_threads_and_init_comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0c90f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class RLDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path, responses_per_prompt):\n",
    "\n",
    "        self.dataset = load_dataset(data_path, split='train')\n",
    "        self.responses_per_prompt = responses_per_prompt\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        ex = self.dataset[idx]\n",
    "        messages = ex[\"messages\"]\n",
    "        answer = ex[\"answer\"]\n",
    "\n",
    "        return {\n",
    "            \"messages\": messages,\n",
    "            \"answer\": answer\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "\n",
    "        return [\n",
    "            copy.deepcopy(ex)\n",
    "            for ex in batch\n",
    "            for _ in range(self.responses_per_prompt)\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "def setup():\n",
    "    # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = 'cpu' \n",
    "    backend = 'nccl' if device == 'cuda' else 'gloo'\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.set_device(local_rank)\n",
    "    os.environ[\"NCCL_CUMEM_ENABLE\"] = \"0\"\n",
    "    os.environ[\"NCCL_NVLS_ENABLE\"] = \"0\"\n",
    "    # Initialize with explicit parameters\n",
    "    dist.init_process_group(\n",
    "        backend=backend, \n",
    "        world_size=world_size,\n",
    "        rank=rank\n",
    "    )\n",
    "\n",
    "\n",
    "def split_data_list(data_list, mesh):\n",
    "  # we need to scatter this data_list across this mesh group, from local group 0.\n",
    "\n",
    "    rank = mesh.get_local_rank()\n",
    "    size = mesh.size()\n",
    "\n",
    "    if rank == 0:\n",
    "        data_per_ddp = math.ceil(len(data_list)/size)\n",
    "    \n",
    "    lists = [data_list[i * data_per_ddp: (i+1)* data_per_ddp] if rank ==0 else None for i in range(size)]\n",
    "    \n",
    "    lst = [None] # this is the output list\n",
    "    dist.scatter_object_list(lst, lists, src=None, group_src=0, group=mesh.get_group())\n",
    "\n",
    "    # print(f'rank {dist.get_rank()} got this list{lst}' )\n",
    "    return lst[0]\n",
    "\n",
    "\n",
    "def gather_data_list(data_list, mesh):\n",
    "    # we need to scatter this data_list across this mesh group, from local group 0.\n",
    "\n",
    "    rank = mesh.get_local_rank()\n",
    "    size = mesh.size()\n",
    "\n",
    "    lists = [None for i in range(size)] if rank==0 else None # Must be None on non-dst ranks otherwise it will call dist.gather_object in other ranks as well is None, it will be called only in the group_dst rank\n",
    "\n",
    "\n",
    "    dist.gather_object(data_list,lists, group_dst=0, group=mesh.get_group()) \n",
    "\n",
    "    #   print(f'rank {dist.get_rank()} got this list{lists}' )\n",
    "    return sum(lists, []) if rank==0 else None # if not group destination, lists wil be None, won't sum\n",
    "\n",
    "\n",
    "\n",
    "def check_mem_allocated(rank, msg):\n",
    "    ans = torch.cuda.memory_allocated() / (1024**3)\n",
    "    print(f'RANK {rank} MEMORY_ALLOCATED {msg} {ans}')\n",
    "\n",
    "\n",
    "def start():\n",
    "    \n",
    "    device = \"cpu\" \n",
    "    mesh = init_device_mesh(device, (2,2,2), mesh_dim_names = ['DDP','FSDP', 'TP'])\n",
    "\n",
    "    train_data = RLDataset(\"CohleM/olympiad_small\", 4)\n",
    "    train_dataloader = StatefulDataLoader(train_data, batch_size=3, drop_last=True, collate_fn=train_data.collate_fn)\n",
    "    # construct train dataloader\n",
    "    \n",
    "    for train_batch in train_dataloader:\n",
    "        if mesh['TP'].get_local_rank() == 0:\n",
    "            # then split across ddp\n",
    "            data_list = split_data_list(train_batch, mesh=mesh['DDP'])\n",
    "        \n",
    "            print(f'rank {dist.get_rank()} data len {len(data_list)} first element {data_list[0]} \\n')\n",
    "            break\n",
    "\n",
    "    return\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    train_batch_size: int = 64\n",
    "    mini_batch_size: int = 8\n",
    "    model_name: str = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "    ddp_size: int = 2 \n",
    "    tp_size: int = 1 \n",
    "    lr: float = 1e-6\n",
    "    data_path: str = 'CohleM/olympiad_small'\n",
    "    responses_per_prompt: int = 4\n",
    "    per_rollout_size: int = 3\n",
    "#     train_batch_size: int = 64\n",
    "#  \n",
    "# def main():\n",
    "#     # setup process groups.\n",
    "#     setup()\n",
    "\n",
    "#     # Initialize ppo trainer with some config\n",
    "#     start()\n",
    "#     dist.destroy_process_group()\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15a943db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@spawn_threads_and_init_comms\n",
    "def shard_big_tensor(world_size):\n",
    "    \n",
    "    device = \"cpu\" \n",
    "    model_device_mesh = init_device_mesh(device, (1,2,2), mesh_dim_names = ['DDP','FSDP','TP'])\n",
    "    \n",
    "    device_mesh = init_device_mesh(device, (2,2), mesh_dim_names = ['DP', 'TP'])\n",
    "\n",
    "\n",
    "    train_data = RLDataset(\"CohleM/olympiad_small\", 4)\n",
    "    print(device_mesh)\n",
    "    train_dataloader = StatefulDataLoader(train_data, batch_size=3, drop_last=True, collate_fn=train_data.collate_fn)\n",
    "    # construct train dataloader\n",
    "    \n",
    "#     print(f'rnak {dist.get_rank()} {mesh['dp']}')\n",
    "    data_list = None\n",
    "    for train_batch in train_dataloader:\n",
    "        if device_mesh['TP'].get_local_rank() == 0:\n",
    "            # then split across ddp\n",
    "            data_list = split_data_list(train_batch, mesh=device_mesh['DP'])\n",
    "        \n",
    "\n",
    "            \n",
    "        \n",
    "        if device_mesh['TP'].get_local_rank() == 0:\n",
    "            data_list = gather_data_list(data_list, mesh=device_mesh['DP'])\n",
    "            \n",
    "#             print(f'rank {dist.get_rank()} data len {len(data_list) if isinstance(data_list, list) else None} \\n')\n",
    "#         print(f'rank {dist.get_rank()} data len {len(data_list) if isinstance(data_list, list) else None} \\n')\n",
    "        \n",
    "        # now for the actor \n",
    "        if device_mesh['TP'].get_local_rank() == 0:\n",
    "            # then split across ddp\n",
    "            data_list = split_data_list(train_batch, mesh=device_mesh['DP'])\n",
    "            \n",
    "        data_list = broadcast_data_list(data_list, mesh=device_mesh['TP'])\n",
    "        \n",
    "        print(f'rank {dist.get_rank()} data len {len(data_list) if isinstance(data_list, list) else None} first element {data_list[0]}\\n')\n",
    "\n",
    "        \n",
    "        break\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1770ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def broadcast_data_list(data_list, mesh):\n",
    "\n",
    "    \n",
    "    if mesh.get_local_rank() == 0:\n",
    "        len_data_list = torch.tensor(len(data_list))\n",
    "    else:\n",
    "        len_data_list = torch.tensor(0)\n",
    "    \n",
    "    dist.broadcast(len_data_list, group=mesh.get_group(), group_src=0)\n",
    "    \n",
    "#     print(len_data_list.item())\n",
    "\n",
    "\n",
    "    if mesh.get_local_rank() != 0:\n",
    "        data_list = [None for _ in range(len_data_list)]\n",
    "    \n",
    "    dist.broadcast_object_list(data_list, group=mesh.get_group(), group_src=0)\n",
    "\n",
    "    return data_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6321a8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('DP', 'TP'))\n",
      "DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('DP', 'TP'))\n",
      "DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('DP', 'TP'))\n",
      "DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('DP', 'TP'))\n",
      "rank 0 data len 6 first element {'messages': [{'content': 'Xenia and Sergey play the following game. Xenia thinks of a positive integer $N$ not exceeding 5000. Then she fixes 20 distinct positive integers $a_{1}, a_{2}, \\\\ldots, a_{20}$ such that, for each $k=1,2, \\\\ldots, 20$, the numbers $N$ and $a_{k}$ are congruent modulo $k$. By a move, Sergey tells Xenia a set $S$ of positive integers not exceeding 20 , and she tells him back the set $\\\\left\\\\{a_{k}: k \\\\in S\\\\right\\\\}$ without spelling out which number corresponds to which index. How many moves does Sergey need to determine for sure the number Xenia thought of?', 'role': 'user'}], 'answer': '2'}\n",
      "\n",
      "rank 2 data len 6 first element {'messages': [{'content': 'Given a positive integer $n$, determine the largest real number $\\\\mu$ satisfying the following condition: for every $4 n$-point configuration $C$ in an open unit square $U$, there exists an open rectangle in $U$, whose sides are parallel to those of $U$, which contains exactly one point of $C$, and has an area greater than or equal to $\\\\mu$.', 'role': 'user'}], 'answer': '$\\\\frac{1}{2 n+2}$'}\n",
      "\n",
      "rank 1 data len 6 first element {'messages': [{'content': 'Xenia and Sergey play the following game. Xenia thinks of a positive integer $N$ not exceeding 5000. Then she fixes 20 distinct positive integers $a_{1}, a_{2}, \\\\ldots, a_{20}$ such that, for each $k=1,2, \\\\ldots, 20$, the numbers $N$ and $a_{k}$ are congruent modulo $k$. By a move, Sergey tells Xenia a set $S$ of positive integers not exceeding 20 , and she tells him back the set $\\\\left\\\\{a_{k}: k \\\\in S\\\\right\\\\}$ without spelling out which number corresponds to which index. How many moves does Sergey need to determine for sure the number Xenia thought of?', 'role': 'user'}], 'answer': '2'}\n",
      "\n",
      "rank 3 data len 6 first element {'messages': [{'content': 'Given a positive integer $n$, determine the largest real number $\\\\mu$ satisfying the following condition: for every $4 n$-point configuration $C$ in an open unit square $U$, there exists an open rectangle in $U$, whose sides are parallel to those of $U$, which contains exactly one point of $C$, and has an area greater than or equal to $\\\\mu$.', 'role': 'user'}], 'answer': '$\\\\frac{1}{2 n+2}$'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shard_big_tensor(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3f429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a16f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67970a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8debbe8",
   "metadata": {},
   "source": [
    "# Single layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "91e82472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "955a23dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def split_data_list(data_list, mesh):\n",
    "  # we need to scatter this data_list across this mesh group, from local group 0.\n",
    "\n",
    "  rank = mesh.get_local_rank()\n",
    "  size = mesh.size()\n",
    "\n",
    "  if rank == 0:\n",
    "    data_per_ddp = math.ceil(len(data_list)/size)\n",
    "  \n",
    "  lists = [data_list[i * data_per_ddp: (i+1)* data_per_ddp] if rank ==0 else None for i in range(size)]\n",
    "  \n",
    "  lst = [None] # this is the output list\n",
    "  dist.scatter_object_list(lst, lists, src=None, group_src=0, group=mesh.get_group())\n",
    "\n",
    "  # print(f'rank {dist.get_rank()} got this list{lst}' )\n",
    "  return lst[0]\n",
    "\n",
    "\n",
    "def gather_data_list(data_list, mesh):\n",
    "  # we need to scatter this data_list across this mesh group, from local group 0.\n",
    "\n",
    "  rank = mesh.get_local_rank()\n",
    "  size = mesh.size()\n",
    "\n",
    "  lists = [None for i in range(size)] if rank==0 else None # Must be None on non-dst ranks otherwise it will call dist.gather_object in other ranks as well is None, it will be called only in the group_dst rank\n",
    "  \n",
    "  \n",
    "  dist.gather_object(data_list,lists, group_dst=0, group=mesh.get_group()) \n",
    "\n",
    "  print(f'rank {dist.get_rank()} got this list{lists}' )\n",
    "  return lists\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "eee0630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def split_data_list(data_list, mesh):\n",
    "  # we need to scatter this data_list across this mesh group, from local group 0.\n",
    "\n",
    "  rank = mesh.get_local_rank()\n",
    "  size = mesh.size()\n",
    "\n",
    "  if rank == 0:\n",
    "    data_per_ddp = math.ceil(len(data_list)/size)\n",
    "  \n",
    "  lists = [data_list[i * data_per_ddp: (i+1)* data_per_ddp] if rank ==0 else None for i in range(size)]\n",
    "  \n",
    "  lst = [None] # this is the output list\n",
    "  dist.scatter_object_list(lst, lists, src=None, group_src=0, group=mesh.get_group())\n",
    "\n",
    "  # print(f'rank {dist.get_rank()} got this list{lst}' )\n",
    "  return lst[0]\n",
    "\n",
    "\n",
    "def gather_data_list(data_list, mesh):\n",
    "  # we need to scatter this data_list across this mesh group, from local group 0.\n",
    "\n",
    "  rank = mesh.get_local_rank()\n",
    "  size = mesh.size()\n",
    "\n",
    "  lists = [None for i in range(size)] if rank==0 else None # Must be None on non-dst ranks otherwise it will call dist.gather_object in other ranks as well is None, it will be called only in the group_dst rank\n",
    "  \n",
    "  \n",
    "  dist.gather_object(data_list,lists, group_dst=0, group=mesh.get_group()) \n",
    "\n",
    "  print(f'rank {dist.get_rank()} got this list{lists}' )\n",
    "  return lists\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_list = [torch.ones((2,2)), torch.ones((2,2))*2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0cc770",
   "metadata": {},
   "source": [
    "### ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "7740f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@spawn_threads_and_init_comms\n",
    "def shard_big_tensor(world_size):\n",
    "    \n",
    "    device = \"cpu\" \n",
    "    model_device_mesh = init_device_mesh(device, (1,2,2), mesh_dim_names = ['DDP','FSDP','TP'])\n",
    "    \n",
    "    device_mesh = init_device_mesh(device, (2,2), mesh_dim_names = ['dp', 'tp'])\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layer0 = nn.Linear(4,4,bias=False)\n",
    "        def forward(self,x):\n",
    "            return self.layer0(x)\n",
    "        \n",
    "    model = Model()\n",
    "\n",
    "    model.layer0.weight = nn.Parameter(torch.arange(1.,17.).reshape(4,4))\n",
    "    \n",
    "    parallelize_module(\n",
    "        module=model,\n",
    "        device_mesh=model_device_mesh['TP'],\n",
    "        parallelize_plan={\n",
    "            \"layer0\": RowwiseParallel() # since rowwise parallel will replicate the output layouts, each tp rank will get the same outptu.\n",
    "        }\n",
    "    )\n",
    "\n",
    "    data_list = [torch.ones((2,2)), torch.ones((2,2))*2]\n",
    "    x = split_data_list(data_list, device_mesh['dp'])[0]\n",
    "\n",
    "\n",
    "    fully_shard(model.layer0, mesh=model_device_mesh['DDP','FSDP'])\n",
    "    fully_shard(model, mesh=model_device_mesh['DDP','FSDP'])\n",
    "    \n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    out = model(x)\n",
    "    print(out)\n",
    "    dist.barrier()\n",
    "    torch.manual_seed(42)\n",
    "    original = torch.randn_like(out)\n",
    "    loss = (original - out)**2\n",
    "    loss = loss.sum()\n",
    "    \n",
    "#     print(f'rank {dist.get_rank()} loss {loss} ')\n",
    "    \n",
    "    print(f' RANK {dist.get_rank()} x = {x} weights = {model.layer0.weight} \\n\\n')\n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "#     print(f' RANK {dist.get_rank()} x = {x} \\n\\n')\n",
    "#     print(f' RANK {dist.get_rank()} x = {x} weights = {model.layer0.weight.grad} \\n\\n')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "45ee1cce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275] Caught exception: \n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275] Traceback (most recent call last):\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]   File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/testing/_internal/common_distributed.py\", line 1065, in worker\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]     callback()\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]   File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/testing/_internal/common_distributed.py\", line 1088, in <lambda>\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]     threads = _run_test_method_with_multi_threads(world_size, lambda: func(self, *args, **kwargs))\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]                                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]   File \"/var/folders/pv/2c84c3jx48v8485vx92jq6zc0000gn/T/ipykernel_76359/4070643146.py\", line 50, in shard_big_tensor\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]     loss.backward()\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]   File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/_tensor.py\", line 648, in backward\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]     torch.autograd.backward(\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]   File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 353, in backward\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]     _engine_run_backward(\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]   File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]   File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_state.py\", line 287, in _root_post_backward_final_callback\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]     fsdp_param_group.post_backward()\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]   File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py\", line 461, in post_backward\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]     assert all_reduce_event is not None\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275] AssertionError\n",
      "E0815 17:23:01.822000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]  exiting thread 1\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275] Caught exception: \n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275] Traceback (most recent call last):\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]   File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/testing/_internal/common_distributed.py\", line 1065, in worker\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]     callback()\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]   File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/testing/_internal/common_distributed.py\", line 1088, in <lambda>\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]     threads = _run_test_method_with_multi_threads(world_size, lambda: func(self, *args, **kwargs))\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]                                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]   File \"/var/folders/pv/2c84c3jx48v8485vx92jq6zc0000gn/T/ipykernel_76359/4070643146.py\", line 50, in shard_big_tensor\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]     loss.backward()\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]   File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/_tensor.py\", line 648, in backward\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]     torch.autograd.backward(\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]   File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 353, in backward\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]     _engine_run_backward(\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]   File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]   File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_state.py\", line 287, in _root_post_backward_final_callback\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]     fsdp_param_group.post_backward()\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]   File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py\", line 461, in post_backward\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]     assert all_reduce_event is not None\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275] AssertionError\n",
      "E0815 17:23:01.823000 76359 site-packages/torch/testing/_internal/common_distributed.py:1275]  exiting thread 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AsyncCollectiveTensor(tensor([[10., 26., 42., 58.],\n",
      "        [10., 26., 42., 58.]]))\n",
      "AsyncCollectiveTensor(tensor([[ 20.,  52.,  84., 116.],\n",
      "        [ 20.,  52.,  84., 116.]]))\n",
      "AsyncCollectiveTensor(tensor([[10., 26., 42., 58.],\n",
      "        [10., 26., 42., 58.]]))\n",
      "AsyncCollectiveTensor(tensor([[ 20.,  52.,  84., 116.],\n",
      "        [ 20.,  52.,  84., 116.]]))\n",
      " RANK 1 x = tensor([[1., 1.],\n",
      "        [1., 1.]]) weights = DTensor(local_tensor=tensor([[3., 4.],\n",
      "        [7., 8.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), Shard(dim=0), Shard(dim=1))) \n",
      "\n",
      "\n",
      " RANK 3 x = tensor([[2., 2.],\n",
      "        [2., 2.]]) weights = DTensor(local_tensor=tensor([[11., 12.],\n",
      "        [15., 16.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), Shard(dim=0), Shard(dim=1))) \n",
      "\n",
      "\n",
      " RANK 0 x = tensor([[1., 1.],\n",
      "        [1., 1.]]) weights = DTensor(local_tensor=tensor([[1., 2.],\n",
      "        [5., 6.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), Shard(dim=0), Shard(dim=1))) \n",
      "\n",
      "\n",
      " RANK 2 x = tensor([[2., 2.],\n",
      "        [2., 2.]]) weights = DTensor(local_tensor=tensor([[ 9., 10.],\n",
      "        [13., 14.]]), device_mesh=DeviceMesh('cpu', [[[0, 1], [2, 3]]], mesh_dim_names=('DDP', 'FSDP', 'TP')), placements=(Replicate(), Shard(dim=0), Shard(dim=1))) \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Thread 1 exited with exception:\nTraceback (most recent call last):\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/testing/_internal/common_distributed.py\", line 1065, in worker\n    callback()\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/testing/_internal/common_distributed.py\", line 1088, in <lambda>\n    threads = _run_test_method_with_multi_threads(world_size, lambda: func(self, *args, **kwargs))\n                                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/pv/2c84c3jx48v8485vx92jq6zc0000gn/T/ipykernel_76359/4070643146.py\", line 50, in shard_big_tensor\n    loss.backward()\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/_tensor.py\", line 648, in backward\n    torch.autograd.backward(\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 353, in backward\n    _engine_run_backward(\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_state.py\", line 287, in _root_post_backward_final_callback\n    fsdp_param_group.post_backward()\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py\", line 461, in post_backward\n    assert all_reduce_event is not None\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n\nThread 3 exited with exception:\nTraceback (most recent call last):\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/testing/_internal/common_distributed.py\", line 1065, in worker\n    callback()\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/testing/_internal/common_distributed.py\", line 1088, in <lambda>\n    threads = _run_test_method_with_multi_threads(world_size, lambda: func(self, *args, **kwargs))\n                                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/pv/2c84c3jx48v8485vx92jq6zc0000gn/T/ipykernel_76359/4070643146.py\", line 50, in shard_big_tensor\n    loss.backward()\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/_tensor.py\", line 648, in backward\n    torch.autograd.backward(\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 353, in backward\n    _engine_run_backward(\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_state.py\", line 287, in _root_post_backward_final_callback\n    fsdp_param_group.post_backward()\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py\", line 461, in post_backward\n    assert all_reduce_event is not None\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[465], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m shard_big_tensor(\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/testing/_internal/common_distributed.py:1090\u001b[0m, in \u001b[0;36mspawn_threads_and_init_comms.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1088\u001b[0m     threads \u001b[38;5;241m=\u001b[39m _run_test_method_with_multi_threads(world_size, \u001b[38;5;28;01mlambda\u001b[39;00m: func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;66;03m# join and error handling\u001b[39;00m\n\u001b[0;32m-> 1090\u001b[0m     MultiThreadedTestCase\u001b[38;5;241m.\u001b[39m_join_threads(threads, func)\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1092\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_distributed_c10d\u001b[38;5;241m.\u001b[39m_set_thread_isolation_mode(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/testing/_internal/common_distributed.py:1250\u001b[0m, in \u001b[0;36mMultiThreadedTestCase._join_threads\u001b[0;34m(cls, threads, fn)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     _uninstall_threaded_pg()\n\u001b[1;32m   1248\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_distributed_c10d\u001b[38;5;241m.\u001b[39m_set_thread_isolation_mode(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1250\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return_codes(failed_ranks, timeout, fn)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/testing/_internal/common_distributed.py:1287\u001b[0m, in \u001b[0;36mMultiThreadedTestCase._check_return_codes\u001b[0;34m(cls, failed_ranks, timeout, fn)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# check exceptions\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msg) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;66;03m# check skip\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_code \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Thread 1 exited with exception:\nTraceback (most recent call last):\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/testing/_internal/common_distributed.py\", line 1065, in worker\n    callback()\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/testing/_internal/common_distributed.py\", line 1088, in <lambda>\n    threads = _run_test_method_with_multi_threads(world_size, lambda: func(self, *args, **kwargs))\n                                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/pv/2c84c3jx48v8485vx92jq6zc0000gn/T/ipykernel_76359/4070643146.py\", line 50, in shard_big_tensor\n    loss.backward()\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/_tensor.py\", line 648, in backward\n    torch.autograd.backward(\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 353, in backward\n    _engine_run_backward(\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_state.py\", line 287, in _root_post_backward_final_callback\n    fsdp_param_group.post_backward()\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py\", line 461, in post_backward\n    assert all_reduce_event is not None\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n\nThread 3 exited with exception:\nTraceback (most recent call last):\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/testing/_internal/common_distributed.py\", line 1065, in worker\n    callback()\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/testing/_internal/common_distributed.py\", line 1088, in <lambda>\n    threads = _run_test_method_with_multi_threads(world_size, lambda: func(self, *args, **kwargs))\n                                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/pv/2c84c3jx48v8485vx92jq6zc0000gn/T/ipykernel_76359/4070643146.py\", line 50, in shard_big_tensor\n    loss.backward()\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/_tensor.py\", line 648, in backward\n    torch.autograd.backward(\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 353, in backward\n    _engine_run_backward(\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_state.py\", line 287, in _root_post_backward_final_callback\n    fsdp_param_group.post_backward()\n  File \"/Users/cohlem/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py\", line 461, in post_backward\n    assert all_reduce_event is not None\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n\n"
     ]
    }
   ],
   "source": [
    "shard_big_tensor(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "63203db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@spawn_threads_and_init_comms\n",
    "def shard_big_tensor(world_size):\n",
    "    \n",
    "    device = \"cpu\" \n",
    "    model_device_mesh = init_device_mesh(device, (2,2), mesh_dim_names = ['FSDP','TP'])\n",
    "    \n",
    "    device_mesh = init_device_mesh(device, (2,2), mesh_dim_names = ['dp', 'tp'])\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layer0 = nn.Linear(4,4,bias=False)\n",
    "        def forward(self,x):\n",
    "            return self.layer0(x)\n",
    "        \n",
    "    model = Model()\n",
    "\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    model.layer0.weight = nn.Parameter(torch.arange(1.,17.).reshape(4,4))\n",
    "    \n",
    "    parallelize_module(\n",
    "        module=model,\n",
    "        device_mesh=model_device_mesh['TP'],\n",
    "        parallelize_plan={\n",
    "            \"layer0\": RowwiseParallel() # since rowwise parallel will replicate the output layouts, each tp rank will get the same outptu.\n",
    "        }\n",
    "    )\n",
    "\n",
    "    \n",
    "    x = split_data_list(data_list, device_mesh['dp'])[0]\n",
    "    \n",
    "\n",
    "    fully_shard(model, mesh=model_device_mesh['FSDP'])\n",
    "    \n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "    \n",
    "    out = model(x)\n",
    "    dist.barrier()\n",
    "\n",
    "#     print(f'RANK {dist.get_rank()}', model_device_mesh['FSDP'])\n",
    "#     print(f'RANK {dist.get_rank()}', device_mesh['dp'])\n",
    "\n",
    "#     print(f' RANK {dist.get_rank()} x = {x} \\n\\n')\n",
    "    loss = out.sum()\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    print(f' RANK {dist.get_rank()} x = {x} weights = {out.wait()} \\n\\n')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "08b0e8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layer0): Linear(in_features=4, out_features=4, bias=False)\n",
      ")Model(\n",
      "  (layer0): Linear(in_features=4, out_features=4, bias=False)\n",
      ")\n",
      "Model(\n",
      "  (layer0): Linear(in_features=4, out_features=4, bias=False)\n",
      ")\n",
      "Model(\n",
      "  (layer0): Linear(in_features=4, out_features=4, bias=False)\n",
      ")\n",
      "\n",
      "RANK 3 DeviceMesh('cpu', [1, 3], mesh_dim_names=('FSDP',))\n",
      "RANK 0 DeviceMesh('cpu', [0, 2], mesh_dim_names=('FSDP',))\n",
      "RANK 2 DeviceMesh('cpu', [0, 2], mesh_dim_names=('FSDP',))\n",
      "RANK 1 DeviceMesh('cpu', [1, 3], mesh_dim_names=('FSDP',))\n",
      "RANK 3 DeviceMesh('cpu', [1, 3], mesh_dim_names=('dp',))\n",
      "RANK 1 DeviceMesh('cpu', [1, 3], mesh_dim_names=('dp',))\n",
      "RANK 2 DeviceMesh('cpu', [0, 2], mesh_dim_names=('dp',))\n",
      "RANK 0 DeviceMesh('cpu', [0, 2], mesh_dim_names=('dp',))\n",
      " RANK 0 x = tensor([[1., 1.],\n",
      "        [1., 1.]]) \n",
      "\n",
      "\n",
      " RANK 2 x = tensor([[2., 2.],\n",
      "        [2., 2.]]) \n",
      "\n",
      "\n",
      " RANK 1 x = tensor([[1., 1.],\n",
      "        [1., 1.]]) \n",
      "\n",
      "\n",
      " RANK 3 x = tensor([[2., 2.],\n",
      "        [2., 2.]]) \n",
      "\n",
      "\n",
      " RANK 1 x = tensor([[1., 1.],\n",
      "        [1., 1.]]) weights = tensor([[10., 26., 42., 58.],\n",
      "        [10., 26., 42., 58.]]) \n",
      "\n",
      "\n",
      " RANK 0 x = tensor([[1., 1.],\n",
      "        [1., 1.]]) weights = tensor([[10., 26., 42., 58.],\n",
      "        [10., 26., 42., 58.]]) \n",
      "\n",
      "\n",
      " RANK 2 x = tensor([[2., 2.],\n",
      "        [2., 2.]]) weights = tensor([[ 20.,  52.,  84., 116.],\n",
      "        [ 20.,  52.,  84., 116.]]) \n",
      "\n",
      "\n",
      " RANK 3 x = tensor([[2., 2.],\n",
      "        [2., 2.]]) weights = tensor([[ 20.,  52.,  84., 116.],\n",
      "        [ 20.,  52.,  84., 116.]]) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shard_big_tensor(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "b3536984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1., 1.],\n",
       "         [1., 1.]]),\n",
       " tensor([[2., 2.],\n",
       "         [2., 2.]])]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "6205e365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributed import ReduceOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "99f014ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [torch.ones((2,2)), torch.ones((2,2))*2]\n",
    "\n",
    "@spawn_threads_and_init_comms\n",
    "def shard_big_tensor(world_size):\n",
    "    \n",
    "    device = \"cpu\" \n",
    "    model_device_mesh = init_device_mesh(device, (2,2), mesh_dim_names = ['FSDP','TP'])\n",
    "    \n",
    "    device_mesh = init_device_mesh(device, (2,2), mesh_dim_names = ['dp', 'tp'])\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layer0 = nn.Linear(4,4,bias=False)\n",
    "        def forward(self,x):\n",
    "            return self.layer0(x)\n",
    "        \n",
    "    model = Model()\n",
    "    \n",
    "    model.layer0.weight = nn.Parameter(torch.arange(1.,17.).reshape(4,4))\n",
    "    \n",
    "    parallelize_module(\n",
    "        module=model,\n",
    "        device_mesh=model_device_mesh['TP'],\n",
    "        parallelize_plan={\n",
    "            \"layer0\": RowwiseParallel() # since rowwise parallel will replicate the output layouts, each tp rank will get the same outptu.\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    \n",
    "    if model_device_mesh['FSDP'].get_local_rank() ==0: # give one data\n",
    "        print(f'rank {dist.get_rank()} x = {data_list[0]}')\n",
    "        out0 = model(data_list[0])\n",
    "#         print(out0)\n",
    "        dist.barrier()\n",
    "        torch.manual_seed(42)\n",
    "        original = torch.randn_like(out0)\n",
    "        loss = (original - out0)**2\n",
    "        loss = loss.sum()\n",
    "        print(f'rank {dist.get_rank()} loss: {loss}')\n",
    "        loss.backward()\n",
    "    \n",
    "    dist.barrier()\n",
    "#     print(f' RANK {dist.get_rank()} before second weights = {(model.layer0.weight.grad)} \\n\\n')\n",
    "    \n",
    "    if model_device_mesh['FSDP'].get_local_rank() ==1: # give one data\n",
    "        print(f'rank {dist.get_rank()} x = {data_list[1]}')\n",
    "        \n",
    "        out1 = model(data_list[1])\n",
    "#         print(out1)\n",
    "        dist.barrier()\n",
    "        torch.manual_seed(42)\n",
    "        original = torch.randn_like(out1)\n",
    "        loss = (original - out1)**2\n",
    "        loss = loss.sum()\n",
    "        print(f'rank {dist.get_rank()} loss: {loss}')\n",
    "        loss.backward()\n",
    "\n",
    "    dist.barrier()\n",
    "    \n",
    "#     print(f' RANK {dist.get_rank()} after second weights = {model.layer0.weight.grad} \\n\\n')\n",
    "\n",
    "    \n",
    "    dt = model.layer0.weight.grad.to_local()\n",
    "\n",
    "    \n",
    "\n",
    "    dist.all_reduce(dt, op=ReduceOp.SUM, group=model_device_mesh['FSDP'].get_group())\n",
    "\n",
    "    print(f' RANK {dist.get_rank()} reduced weights = {dt/2} \\n\\n')  \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "f934e77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 0 x = tensor([[1., 1.],\n",
      "        [1., 1.]])rank 1 x = tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "rank 1 loss: 11675.63671875\n",
      "rank 0 loss: 11675.63671875\n",
      "rank 3 x = tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "rank 2 x = tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "rank 3 loss: 46960.45703125\n",
      "rank 2 loss: 46960.45703125\n",
      " RANK 1 reduced weights = tensor([[102.3585, 102.3585],\n",
      "        [260.1725, 260.1725],\n",
      "        [412.6720, 412.6720],\n",
      "        [581.2230, 581.2230]]) \n",
      "\n",
      "\n",
      " RANK 2 reduced weights = tensor([[102.3585, 102.3585],\n",
      "        [260.1725, 260.1725],\n",
      "        [412.6720, 412.6720],\n",
      "        [581.2230, 581.2230]]) \n",
      "\n",
      "\n",
      " RANK 3 reduced weights = tensor([[102.3585, 102.3585],\n",
      "        [260.1725, 260.1725],\n",
      "        [412.6720, 412.6720],\n",
      "        [581.2230, 581.2230]]) \n",
      "\n",
      "\n",
      " RANK 0 reduced weights = tensor([[102.3585, 102.3585],\n",
      "        [260.1725, 260.1725],\n",
      "        [412.6720, 412.6720],\n",
      "        [581.2230, 581.2230]]) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shard_big_tensor(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "dcd0c7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1., 1.],\n",
       "         [1., 1.]]),\n",
       " tensor([[2., 2.],\n",
       "         [2., 2.]])]"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "2e5a0c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [torch.ones((2,2)), torch.ones((2,2))*2]\n",
    "\n",
    "@spawn_threads_and_init_comms\n",
    "def shard_big_tensor(world_size):\n",
    "    \n",
    "    device = \"cpu\" \n",
    "    model_device_mesh = init_device_mesh(device, (2,2), mesh_dim_names = ['FSDP','TP'])\n",
    "    \n",
    "    device_mesh = init_device_mesh(device, (2,2), mesh_dim_names = ['dp', 'tp'])\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layer0 = nn.Linear(4,4,bias=False)\n",
    "        def forward(self,x):\n",
    "            return self.layer0(x)\n",
    "        \n",
    "    model = Model()\n",
    "\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    model.layer0.weight = nn.Parameter(torch.arange(1.,17.).reshape(4,4))\n",
    "    \n",
    "    parallelize_module(\n",
    "        module=model,\n",
    "        device_mesh=model_device_mesh['TP'],\n",
    "        parallelize_plan={\n",
    "            \"layer0\": RowwiseParallel() # since rowwise parallel will replicate the output layouts, each tp rank will get the same outptu.\n",
    "        }\n",
    "    )\n",
    "\n",
    "    \n",
    "    x = split_data_list(data_list, device_mesh['dp'])[0]\n",
    "    print(f' RANK {dist.get_rank()} x= {x} \\n\\n')\n",
    "    \n",
    "\n",
    "    fully_shard(model, mesh=model_device_mesh['FSDP'])\n",
    "    \n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    out = model(x)\n",
    "    print(out)\n",
    "    dist.barrier()\n",
    "    torch.manual_seed(42)\n",
    "    original = torch.randn_like(out)\n",
    "    loss = (original - out)**2\n",
    "    loss = loss.sum()\n",
    "    \n",
    "    print(f'rank {dist.get_rank()} loss {loss} ')\n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f' RANK {dist.get_rank()} weights = {model.layer0.weight.grad} \\n\\n')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "45134ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layer0): Linear(in_features=4, out_features=4, bias=False)\n",
      ")\n",
      "Model(\n",
      "  (layer0): Linear(in_features=4, out_features=4, bias=False)\n",
      ")\n",
      "Model(\n",
      "  (layer0): Linear(in_features=4, out_features=4, bias=False)\n",
      ")\n",
      "Model(\n",
      "  (layer0): Linear(in_features=4, out_features=4, bias=False)\n",
      ")\n",
      " RANK 2 x= tensor([[2., 2.],\n",
      "        [2., 2.]]) \n",
      "\n",
      "\n",
      " RANK 0 x= tensor([[1., 1.],\n",
      "        [1., 1.]]) \n",
      "\n",
      "\n",
      " RANK 3 x= tensor([[2., 2.],\n",
      "        [2., 2.]]) \n",
      "\n",
      "\n",
      " RANK 1 x= tensor([[1., 1.],\n",
      "        [1., 1.]]) \n",
      "\n",
      "\n",
      "AsyncCollectiveTensor(tensor([[10., 26., 42., 58.],\n",
      "        [10., 26., 42., 58.]]))\n",
      "AsyncCollectiveTensor(tensor([[ 20.,  52.,  84., 116.],\n",
      "        [ 20.,  52.,  84., 116.]]))\n",
      "AsyncCollectiveTensor(tensor([[ 20.,  52.,  84., 116.],\n",
      "        [ 20.,  52.,  84., 116.]]))\n",
      "AsyncCollectiveTensor(tensor([[10., 26., 42., 58.],\n",
      "        [10., 26., 42., 58.]]))\n",
      "rank 1 loss 11675.63671875 \n",
      "rank 2 loss 46960.45703125 \n",
      "rank 0 loss 11675.63671875 \n",
      "rank 3 loss 46960.45703125 \n",
      " RANK 0 weights = DTensor(local_tensor=tensor([[102.3585, 102.3585],\n",
      "        [260.1725, 260.1725]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(Shard(dim=0), Shard(dim=1))) \n",
      "\n",
      "\n",
      " RANK 2 weights = DTensor(local_tensor=tensor([[412.6720, 412.6720],\n",
      "        [581.2230, 581.2230]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(Shard(dim=0), Shard(dim=1))) \n",
      "\n",
      "\n",
      " RANK 1 weights = DTensor(local_tensor=tensor([[102.3585, 102.3585],\n",
      "        [260.1725, 260.1725]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(Shard(dim=0), Shard(dim=1))) \n",
      "\n",
      "\n",
      " RANK 3 weights = DTensor(local_tensor=tensor([[412.6720, 412.6720],\n",
      "        [581.2230, 581.2230]]), device_mesh=DeviceMesh('cpu', [[0, 1], [2, 3]], mesh_dim_names=('FSDP', 'TP')), placements=(Shard(dim=0), Shard(dim=1))) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shard_big_tensor(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "b3261a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layer0): Linear(in_features=4, out_features=4, bias=False)\n",
      ")\n",
      "weights = tensor([[102.3585, 102.3585, 102.3585, 102.3585],\n",
      "        [260.1725, 260.1725, 260.1725, 260.1725],\n",
      "        [412.6720, 412.6720, 412.6720, 412.6720],\n",
      "        [581.2230, 581.2230, 581.2230, 581.2230]]) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = [torch.ones((2,4)), torch.ones((2,4))*2]\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer0 = nn.Linear(4,4,bias=False)\n",
    "    def forward(self,x):\n",
    "        return self.layer0(x)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "\n",
    "print(model)\n",
    "\n",
    "model.layer0.weight = nn.Parameter(torch.arange(1.,17.).reshape(4,4))\n",
    "\n",
    "\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "optim.zero_grad(set_to_none=True)\n",
    "out0 = model(data_list[0])\n",
    "torch.manual_seed(42)\n",
    "original = torch.randn_like(out0)\n",
    "loss = (original - out0)**2\n",
    "loss = loss.sum()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "out1 = model(data_list[1])\n",
    "torch.manual_seed(42)\n",
    "original = torch.randn_like(out1)\n",
    "loss = (original - out1)**2\n",
    "loss = loss.sum()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "    \n",
    "print(f'weights = {model.layer0.weight.grad/2} \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "08a80565",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m), torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m5\u001b[39m)])\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "pos_emb = torch.tensor(torch.randn(2,4), torch.randn(2,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c93fb09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7550b78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.0605,  0.4071],\n",
       "         [ 1.3052,  0.4336],\n",
       "         [-1.0205,  1.7036]])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.randn(3,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8df3b71",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mtensor([torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m)])\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "torch.tensor([torch.randn(2,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e19d098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16505d5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/cohlem/Downloads/pkls/minibatches_.pkl0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     minibatch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/serialization.py:1549\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1547\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(\n\u001b[1;32m   1550\u001b[0m     opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args\n\u001b[1;32m   1551\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/serialization.py:1797\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1794\u001b[0m         \u001b[38;5;66;03m# if not a tarfile, reset file offset and proceed\u001b[39;00m\n\u001b[1;32m   1795\u001b[0m         f\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 1797\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[1;32m   1799\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/storage.py:530\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_from_bytes\u001b[39m(b):\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(io\u001b[38;5;241m.\u001b[39mBytesIO(b), weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/serialization.py:1549\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1547\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(\n\u001b[1;32m   1550\u001b[0m     opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args\n\u001b[1;32m   1551\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/serialization.py:1807\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1805\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1806\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1807\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m   1809\u001b[0m deserialized_storage_keys \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mactive_fake_mode() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _serialization_tls\u001b[38;5;241m.\u001b[39mskip_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/serialization.py:1742\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1740\u001b[0m     obj \u001b[38;5;241m=\u001b[39m cast(Storage, torch\u001b[38;5;241m.\u001b[39mUntypedStorage(nbytes))\n\u001b[1;32m   1741\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_torch_load_uninitialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1742\u001b[0m     obj \u001b[38;5;241m=\u001b[39m restore_location(obj, location)\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1745\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m   1746\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39mobj, dtype\u001b[38;5;241m=\u001b[39mdtype, _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/serialization.py:698\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 698\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(storage, location)\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/serialization.py:636\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    634\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[0;32m--> 636\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/serialization.py:605\u001b[0m, in \u001b[0;36m_validate_device\u001b[0;34m(location, backend_name)\u001b[0m\n\u001b[1;32m    603\u001b[0m     device_index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_available\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_available() is False. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    608\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    611\u001b[0m     )\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_count\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    613\u001b[0m     device_count \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "with open('/Users/cohlem/Downloads/pkls/minibatches_.pkl0', 'rb') as f:\n",
    "    minibatch = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "38ab8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ppo_loss = torch.randint(0,5, (2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "92aec88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 3, 3],\n",
       "        [1, 0, 4]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4582245c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_loss.sum(dim=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c4c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
