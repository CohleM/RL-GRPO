{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aefd307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f536bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_list_.pkl0    data_list_.pkl2    minibatches_.pkl0  minibatches_.pkl2\r\n",
      "data_list_.pkl1    data_list_.pkl3    minibatches_.pkl1  minibatches_.pkl3\r\n"
     ]
    }
   ],
   "source": [
    "ls pkls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab4ad8a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpkls/minibatches_.pkl\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 6\u001b[0m         minibatch\u001b[38;5;241m.\u001b[39mappend(pickle\u001b[38;5;241m.\u001b[39mload(f))\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpkls/data_list_.pkl\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m         minibatch\u001b[38;5;241m.\u001b[39mappend(pickle\u001b[38;5;241m.\u001b[39mload(f))\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/storage.py:530\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_from_bytes\u001b[39m(b):\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(io\u001b[38;5;241m.\u001b[39mBytesIO(b), weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/serialization.py:1549\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1547\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(\n\u001b[1;32m   1550\u001b[0m     opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args\n\u001b[1;32m   1551\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/serialization.py:1807\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1805\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1806\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1807\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m   1809\u001b[0m deserialized_storage_keys \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mactive_fake_mode() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _serialization_tls\u001b[38;5;241m.\u001b[39mskip_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/serialization.py:1742\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1740\u001b[0m     obj \u001b[38;5;241m=\u001b[39m cast(Storage, torch\u001b[38;5;241m.\u001b[39mUntypedStorage(nbytes))\n\u001b[1;32m   1741\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_torch_load_uninitialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1742\u001b[0m     obj \u001b[38;5;241m=\u001b[39m restore_location(obj, location)\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1745\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m   1746\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39mobj, dtype\u001b[38;5;241m=\u001b[39mdtype, _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/serialization.py:698\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 698\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(storage, location)\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/serialization.py:636\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    634\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[0;32m--> 636\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.12/site-packages/torch/serialization.py:605\u001b[0m, in \u001b[0;36m_validate_device\u001b[0;34m(location, backend_name)\u001b[0m\n\u001b[1;32m    603\u001b[0m     device_index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_available\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_available() is False. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    608\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    611\u001b[0m     )\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_count\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    613\u001b[0m     device_count \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "minibatch = []\n",
    "data_list = []\n",
    "for i in range(4):\n",
    "\n",
    "    with open(f'pkls/minibatches_.pkl{i}', 'rb') as f:\n",
    "        minibatch.append(pickle.load(f))\n",
    "\n",
    "    with open(f'pkls/data_list_.pkl{i}', 'rb') as f:\n",
    "        minibatch.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2dd2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd84bd3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (236) must match the size of tensor b (118) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m236\u001b[39m, \u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m      2\u001b[0m cos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m118\u001b[39m, \u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m q \u001b[38;5;241m*\u001b[39m cos\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (236) must match the size of tensor b (118) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "q = torch.rand(6,4,236, 128)\n",
    "cos = torch.rand(6,4,118, 128)\n",
    "\n",
    "q * cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d005affe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ee03c05",
   "metadata": {},
   "source": [
    "### actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b89cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.distributed.checkpoint.state_dict import (\n",
    "    StateDictOptions, get_model_state_dict\n",
    ")\n",
    "from transformers import AutoModelForCausalLM\n",
    "from RL2.workers import Worker\n",
    "from RL2.utils.sequences import count_total\n",
    "from RL2.utils.ring_attn import update_params_of_ring_attn\n",
    "from RL2.utils.functions import (\n",
    "    compute_logsumexp,\n",
    "    gather_action_logits,\n",
    "    compute_entropy\n",
    ")\n",
    "from RL2.utils.algorithms import (\n",
    "    compute_approx_kl, compute_surrogate_loss\n",
    ")\n",
    "from RL2.utils.offloading import load_model_to_device\n",
    "from RL2.utils.logging import (\n",
    "    progress_bar,\n",
    "    time_logger,\n",
    "    gather_and_reduce,\n",
    "    rank0_log\n",
    ")\n",
    "\n",
    "\n",
    "class Actor(Worker):\n",
    "\n",
    "    def __init__(self, config, train: bool):\n",
    "        super().__init__(config, train)\n",
    "        \n",
    "        if config.use_liger_kernel:\n",
    "            assert config.tp_size == 1, \\\n",
    "                \"Liger kernel is not compatible with tensor parallelism.\"\n",
    "            from liger_kernel.transformers import AutoLigerKernelForCausalLM\n",
    "            model_cls = AutoLigerKernelForCausalLM\n",
    "        else:\n",
    "            model_cls = AutoModelForCausalLM\n",
    "\n",
    "        self.model = model_cls.from_pretrained(\n",
    "            config.model_name,\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation=\"flash_attention_2\"\n",
    "        )\n",
    "\n",
    "        self.prepare_model_optimizer()\n",
    "\n",
    "    def forward(self, minibatch, return_entropy=False):\n",
    "        update_params_of_ring_attn(\n",
    "            minibatch[\"cu_seqlens\"], self.device_mesh[\"sp\"]\n",
    "        )\n",
    "\n",
    "        logits = self.model(\n",
    "            input_ids=minibatch[\"states\"],\n",
    "            position_ids=minibatch[\"position_ids\"],\n",
    "            use_cache=False\n",
    "        ).logits.to(torch.float32) / getattr(\n",
    "            self.config, \"temperature\", 1.0\n",
    "        )\n",
    "        # bfloat16 is unstable for the subsequent `logsumexp` operation.\n",
    "        # See https://github.com/OpenRLHF/OpenRLHF/pull/634.\n",
    "        \n",
    "        logsumexp = compute_logsumexp(logits, self.device_mesh[\"tp\"])\n",
    "        action_logits = gather_action_logits(\n",
    "            logits,\n",
    "            minibatch[\"actions\"],\n",
    "            self.device_mesh[\"tp\"]\n",
    "        )\n",
    "        logps = (action_logits - logsumexp) * minibatch[\"action_mask\"]\n",
    "        \n",
    "        if return_entropy:\n",
    "            entropy = compute_entropy(\n",
    "                logits, logsumexp, self.device_mesh[\"tp\"]\n",
    "            ) * minibatch[\"action_mask\"]\n",
    "            return logps, entropy\n",
    "        else:\n",
    "            return logps\n",
    "\n",
    "    @time_logger(\"compute_logps\")\n",
    "    @torch.no_grad()\n",
    "    def compute_logps(self, data_list, step):\n",
    "        load_model_to_device(self, torch.cuda.current_device())\n",
    "        \n",
    "        len_data_list = len(data_list) if isinstance(data_list, list) else None\n",
    "        \n",
    "        minibatches = self.scatter_and_pack_data_list(data_list)\n",
    "\n",
    "        import pickle\n",
    "\n",
    "        if data_list is not None:\n",
    "                \n",
    "            with open(f\"data_list_.pkl{dist.get_rank()}\", \"wb\") as f:\n",
    "                pickle.dump(data_list, f)\n",
    "            \n",
    "        with open(f\"minibatches_.pkl{dist.get_rank()}\", \"wb\") as f:\n",
    "            pickle.dump(minibatches, f)\n",
    "        \n",
    "        print(f' RANK {dist.get_rank()} before scatter {len_data_list} mini_batches {len(minibatches)}')\n",
    "        prefix = \"old\" if self.train else \"ref\"\n",
    "\n",
    "        self.model.eval()\n",
    "        for minibatch in progress_bar(\n",
    "            minibatches, desc=f\"Compute {prefix} logps\"\n",
    "        ):\n",
    "            minibatch[f\"{prefix}_logps\"] = self.forward(minibatch)\n",
    "        \n",
    "        if not self.train:\n",
    "            load_model_to_device(self, \"cpu\")\n",
    "        return self.unpack_and_gather_data_list(minibatches) \n",
    "    \n",
    "    @time_logger(\"update_actor\")\n",
    "    def update(self, data_list, step: int):\n",
    "        \n",
    "        if step < self.config.freeze_steps:\n",
    "            load_model_to_device(self, \"cpu\")\n",
    "            return\n",
    "        load_model_to_device(self, torch.cuda.current_device())\n",
    "        batches = self.scatter_and_pack_data_list(data_list, True)\n",
    "\n",
    "        self.model.train()\n",
    "        tbar = progress_bar(\n",
    "            total=sum([len(batch) for batch in batches]),\n",
    "            desc=\"Update actor\"\n",
    "        )\n",
    "        metrics = defaultdict(list)\n",
    "        for batch in batches:\n",
    "            \n",
    "            total_actions = count_total(batch, \"action_mask\", self.device_mesh)\n",
    "            total_sequences = count_total(batch, \"eos_mask\", self.device_mesh)\n",
    "            metric = defaultdict(list)\n",
    "            for minibatch in batch:\n",
    "\n",
    "                logps, entropy = self.forward(minibatch, return_entropy=True)\n",
    "                surrogate_loss, clip_ratio = compute_surrogate_loss(\n",
    "                    self, logps, minibatch, total_actions, total_sequences\n",
    "                )\n",
    "                entropy_loss = - entropy.sum() / total_actions\n",
    "                loss = surrogate_loss + self.config.entropy.coef * entropy_loss\n",
    "\n",
    "                if self.config.kl.coef > 0 and self.config.kl.type == \"loss\":\n",
    "                    kl_loss = compute_approx_kl(\n",
    "                        logps,\n",
    "                        minibatch[\"ref_logps\"],\n",
    "                        self.config.kl.loss_estimator\n",
    "                    ).sum() / total_actions\n",
    "                    loss = loss + self.config.kl.coef * kl_loss\n",
    "\n",
    "                self.backward(loss)\n",
    "\n",
    "                tbar.update()\n",
    "                metric[\"actor/entropy_loss\"].append(entropy_loss.item())\n",
    "                metric[\"actor/loss\"].append(loss.item())\n",
    "                metric[\"actor/clip_ratio\"].append(clip_ratio.item())\n",
    "\n",
    "            grad_norm = self.optimizer_step()\n",
    "\n",
    "            for k, v in metric.items():\n",
    "                metrics[k].append(\n",
    "                    gather_and_reduce(v, self.device_mesh)\n",
    "                )\n",
    "            metrics[\"actor/grad_norm\"].append(grad_norm)\n",
    "\n",
    "        rank0_log(metrics, step)\n",
    "\n",
    "        options = StateDictOptions(full_state_dict=False, cpu_offload=True)\n",
    "        state_dict = get_model_state_dict(\n",
    "            self.model, options=options\n",
    "        )\n",
    "        load_model_to_device(self, \"cpu\")\n",
    "        return state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aeb9b3",
   "metadata": {},
   "source": [
    "### rollout.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7325f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import importlib\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.distributed.tensor import DTensor\n",
    "from sglang.srt.entrypoints.engine import Engine\n",
    "from sglang.srt.patch_torch import monkey_patch_torch_reductions\n",
    "from sglang.srt.utils import MultiprocessingSerializer\n",
    "from sglang.srt.model_executor.model_runner import LocalSerializedTensor\n",
    "from tqdm.asyncio import tqdm\n",
    "import wandb\n",
    "from RL2.workers import Worker\n",
    "from RL2.datasets import tokenize_messages\n",
    "from RL2.utils.comm import split_and_scatter_list, gather_and_concat_list\n",
    "from RL2.utils.logging import time_logger, gather_and_log\n",
    "\n",
    "\n",
    "class Rollout(Worker):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config, None)\n",
    "        \n",
    "        self.prepare_environment_variables()\n",
    "        if self.device_mesh[\"tp\"].get_local_rank() == 0:\n",
    "            self.prepare_environment()\n",
    "\n",
    "            os.environ[\"SGLANG_BLOCK_NONZERO_RANK_CHILDREN\"] = \"0\"\n",
    "            self.llm = Engine(\n",
    "                model_path=config.model_name,\n",
    "                dtype=\"bfloat16\",\n",
    "                tp_size=self.device_mesh[\"tp\"].size(),\n",
    "                mem_fraction_static=config.gpu_memory_utilization,\n",
    "                enable_memory_saver=True,\n",
    "                port=30000 + dist.get_rank()\n",
    "            )\n",
    "        \n",
    "            self.train_sampling_params = OmegaConf.to_container(\n",
    "                config.train_sampling_params\n",
    "            )\n",
    "            self.test_sampling_params = OmegaConf.to_container(\n",
    "                config.test_sampling_params\n",
    "            )\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "    def prepare_device_mesh(self):\n",
    "\n",
    "        world_size = dist.get_world_size()\n",
    "        assert world_size % self.config.tp_size == 0, \\\n",
    "            f\"World_size {world_size} must be divisible by tp_size {self.config.tp_size}.\"\n",
    "        self.dp_size = world_size // self.config.tp_size\n",
    "        self.device_mesh = dist.device_mesh.init_device_mesh(\n",
    "            \"cpu\",\n",
    "            mesh_dim_names=(\"dp\", \"tp\"),\n",
    "            mesh_shape=(self.dp_size, self.config.tp_size)\n",
    "        )\n",
    "\n",
    "    def prepare_environment_variables(self):\n",
    "\n",
    "        if \"TORCHELASTIC_USE_AGENT_STORE\" in os.environ.keys():\n",
    "            del os.environ[\"TORCHELASTIC_USE_AGENT_STORE\"]\n",
    "        monkey_patch_torch_reductions()\n",
    "        cuda_visible_devices = self.device_mesh[\"tp\"].size() * [None]\n",
    "        dist.all_gather_object(\n",
    "            cuda_visible_devices,\n",
    "            os.environ[\"LOCAL_RANK\"],\n",
    "            self.device_mesh[\"tp\"].get_group()\n",
    "        )\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(cuda_visible_devices)\n",
    "\n",
    "    def prepare_environment(self):\n",
    "\n",
    "        spec = importlib.util.spec_from_file_location(\n",
    "            \"custom_module\", self.config.env_path\n",
    "        )\n",
    "        self.env = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(self.env)\n",
    "        \n",
    "    async def rollout(self, ex, train):\n",
    "\n",
    "        messages, answer = ex[\"messages\"], ex[\"answer\"]\n",
    "        metric = defaultdict(list)\n",
    "        for turn in range(self.config.max_turns):\n",
    "\n",
    "            if self.config.apply_chat_template:\n",
    "                prompt = self.tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    add_generation_prompt=True,\n",
    "                    tokenize=False\n",
    "                )\n",
    "            else:\n",
    "                prompt = \"\".join([\n",
    "                    msg[\"content\"] for msg in messages\n",
    "                ])\n",
    "            \n",
    "            response = await self.llm.async_generate(\n",
    "                prompt,\n",
    "                sampling_params=self.train_sampling_params\n",
    "                if train else self.test_sampling_params\n",
    "            )\n",
    "\n",
    "            meta_info = response[\"meta_info\"]\n",
    "            metric[\"response_length\"].append(meta_info[\"completion_tokens\"])\n",
    "            metric[\"length_clip_ratio\"].append(\n",
    "                meta_info[\"finish_reason\"][\"type\"] == \"length\"\n",
    "            )\n",
    "\n",
    "            # Current SGLang engine will generate sequence longer than \n",
    "            # `max_new_tokens`.\n",
    "            # TODO (P1): Check whether all configurations are properly set \n",
    "            # and whether the bug has been fixed in the latest version.\n",
    "            content = self.tokenizer.decode(\n",
    "                self.tokenizer.encode(\n",
    "                    response[\"text\"], add_special_tokens=False\n",
    "                )[:meta_info[\"completion_tokens\"]]\n",
    "            )\n",
    "            messages.append(\n",
    "                {\"role\": \"assistant\", \"content\": content}\n",
    "            )\n",
    "\n",
    "            # Do not invoke tools in the last turn.\n",
    "            if turn + 1 == self.config.max_turns:\n",
    "                break\n",
    "\n",
    "            env_messages = self.env.interact(messages)\n",
    "            # Terminate if no tool is invoked.\n",
    "            if len(env_messages) == 0:\n",
    "                break\n",
    "\n",
    "            messages.extend(env_messages)\n",
    "\n",
    "        reward = self.env.reward_fn(messages, answer)\n",
    "\n",
    "        ex = tokenize_messages(\n",
    "            self.tokenizer,\n",
    "            messages,\n",
    "            self.config.apply_chat_template\n",
    "        )\n",
    "        ex.update({\n",
    "            \"rewards\": torch.FloatTensor((ex[\"states\"].shape[-1] - 1) * [0] + [reward]),\n",
    "            \"eos_mask\": torch.LongTensor((ex[\"states\"].shape[-1] - 1) * [0] + [1])\n",
    "        })\n",
    "\n",
    "        metric[\"n_turns\"].append(turn + 1)\n",
    "        metric[\"rewards\"].append(reward)\n",
    "        metric[\"trajectory_length\"].append(len(ex[\"states\"]))\n",
    "\n",
    "        return ex, messages, metric\n",
    "\n",
    "    @time_logger(\"rollout\")\n",
    "    def __call__(self, data_list, train: bool, step: int):\n",
    "\n",
    "        # The data is distributed from rank 0 before each worker operation\n",
    "        # and gathered before the next operation, which facilitates to do\n",
    "        # model-agnostic operations, e.g., computing advantages, globally \n",
    "        # and guarantees the load balancing across all model computations.\n",
    "        if self.device_mesh[\"tp\"].get_local_rank() == 0:\n",
    "\n",
    "            data_list = split_and_scatter_list(\n",
    "                data_list, self.device_mesh[\"dp\"]\n",
    "            )\n",
    "            # print(f' rank {dist.get_rank()} rollout data list { len(data_list) if isinstance(data_list, list) else None} model weights wq, ')\n",
    "            loop = asyncio.get_event_loop()\n",
    "            outputs = loop.run_until_complete(\n",
    "                tqdm.gather(\n",
    "                    *(self.rollout(ex, train) for ex in data_list),\n",
    "                    desc=\"Rollout\", position=1, leave=False,\n",
    "                    disable=(dist.get_rank() != 0)\n",
    "                )\n",
    "            )\n",
    "            if train:\n",
    "                # If test, llm will soon be called again. See `Trainer.train`.\n",
    "                self.llm.release_memory_occupation()\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "        if self.device_mesh[\"tp\"].get_local_rank() == 0:\n",
    "\n",
    "            data_list, all_messages, metrics = map(list, zip(*outputs))\n",
    "\n",
    "            if dist.get_rank() == 0:\n",
    "                tqdm.write(json.dumps(all_messages[0], indent=4))\n",
    "\n",
    "            suffix = \"train\" if train else \"test\"\n",
    "            metrics = {\n",
    "                f\"{k}/{suffix}\": sum([metric[k] for metric in metrics], [])\n",
    "                for k in metrics[0].keys()\n",
    "            }\n",
    "            gather_and_log(metrics, self.device_mesh[\"dp\"], step)\n",
    "\n",
    "            if not train:\n",
    "                return\n",
    "\n",
    "            data_list = gather_and_concat_list(\n",
    "                data_list, self.device_mesh[\"dp\"]\n",
    "            )\n",
    "\n",
    "            print(f' rank {dist.get_rank()} after collecting data list { len(data_list) if isinstance(data_list, list) else None} model weights wq, ')\n",
    "            if dist.get_rank() == 0:\n",
    "                if not self.config.dynamic_filtering:\n",
    "                    return data_list\n",
    "\n",
    "                rewards = torch.FloatTensor(\n",
    "                    [ex[\"rewards\"].sum() for ex in data_list]\n",
    "                ).view(-1, self.config.responses_per_prompt)\n",
    "                are_filtered = (rewards.std(-1) == 0).tolist()\n",
    "                wandb.log({\n",
    "                    \"dynamic_filtering_ratio\": sum(are_filtered) / len(are_filtered)\n",
    "                }, step=step)\n",
    "                return sum([\n",
    "                    data_list[idx * self.config.responses_per_prompt:(idx + 1) * self.config.responses_per_prompt]\n",
    "                    for idx, is_filtered in enumerate(are_filtered)\n",
    "                    if not is_filtered\n",
    "                ], [])\n",
    "        \n",
    "    @time_logger(\"update_rollout\")\n",
    "    def update(self, state_dict, step):\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        # or llm.resume_memory_occupation() may OOM\n",
    "        if self.device_mesh[\"tp\"].get_local_rank() == 0:\n",
    "            self.llm.resume_memory_occupation()\n",
    "        \n",
    "        for idx, (name, tensor) in enumerate(state_dict.items()):\n",
    "            tensor = tensor.to(torch.cuda.current_device())\n",
    "            serialized_tensor = MultiprocessingSerializer.serialize(\n",
    "                tensor.full_tensor() if isinstance(tensor, DTensor) else tensor\n",
    "            )\n",
    "            serialized_tensors = [\n",
    "                None for _ in range(self.device_mesh[\"tp\"].size())\n",
    "            ] if self.device_mesh[\"tp\"].get_local_rank() == 0 else None\n",
    "            dist.gather_object(\n",
    "                serialized_tensor,\n",
    "                serialized_tensors,\n",
    "                group_dst=0,\n",
    "                group=self.device_mesh[\"tp\"].get_group(),\n",
    "            )\n",
    "            if self.device_mesh[\"tp\"].get_local_rank() == 0:\n",
    "                self.llm.update_weights_from_tensor(\n",
    "                    named_tensors=[(\n",
    "                        name, LocalSerializedTensor(values=serialized_tensors)\n",
    "                    )],\n",
    "                    flush_cache=(idx == len(state_dict) - 1)\n",
    "                )\n",
    "        dist.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c91ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "727084c4",
   "metadata": {},
   "source": [
    "### ppo.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data:\n",
    "  train_data_path: null\n",
    "  test_data_path: null\n",
    "  prompts_per_rollout: null\n",
    "  responses_per_prompt: null\n",
    "  \n",
    "actor:\n",
    "  model_name: null\n",
    "  tokenizer_name: ${actor.model_name}\n",
    "  use_liger_kernel: false\n",
    "  gradient_checkpointing: true\n",
    "  ddp_size: 1\n",
    "  tp_size: 2\n",
    "  sp_size: 1\n",
    "  max_length_per_device: null\n",
    "  max_inference_length_per_device: ${actor.max_length_per_device}\n",
    "  temperature: ${rollout.train_sampling_params.temperature}\n",
    "  update_per_rollout: 3\n",
    "  clip: 0.2\n",
    "  agg_mode: all_token_mean\n",
    "  lr: 1e-6\n",
    "  weight_decay: 1e-2\n",
    "  max_grad_norm: 1.0\n",
    "  scheduler: constant\n",
    "  warmup_ratio: 0.1\n",
    "  freeze_steps: 0\n",
    "  offload_model: true\n",
    "  offload_optimizer: true\n",
    "\n",
    "  kl:\n",
    "    coef: 0.0\n",
    "    type: null # `reward` or `loss`\n",
    "    reward_estimator: k1\n",
    "    loss_estimator: k2\n",
    "    # `k1`, `k2` or `k3`. See http://joschu.net/blog/kl-approx.html.\n",
    "\n",
    "  entropy:\n",
    "    coef: 0.0\n",
    "\n",
    "rollout:\n",
    "  model_name: ${actor.model_name}\n",
    "  tokenizer_name: ${rollout.model_name}\n",
    "  tp_size: 2\n",
    "  gpu_memory_utilization: 0.5\n",
    "  responses_per_prompt: ${data.responses_per_prompt}\n",
    "  apply_chat_template: true\n",
    "  train_sampling_params:\n",
    "    temperature: 1.0\n",
    "    max_new_tokens: null\n",
    "  test_sampling_params:\n",
    "    temperature: 0.0\n",
    "    max_new_tokens: ${rollout.train_sampling_params.max_new_tokens}\n",
    "  max_turns: 1\n",
    "  env_path: null\n",
    "  dynamic_filtering: false\n",
    "\n",
    "ref_actor:\n",
    "  model_name: ${actor.model_name}\n",
    "  tokenizer_name: ${ref_actor.model_name}\n",
    "  use_liger_kernel: ${actor.use_liger_kernel}\n",
    "  ddp_size: ${actor.ddp_size}\n",
    "  tp_size: ${actor.tp_size}\n",
    "  sp_size: ${actor.sp_size}\n",
    "  max_inference_length_per_device: ${actor.max_length_per_device}\n",
    "  temperature: ${rollout.train_sampling_params.temperature}\n",
    "  offload_model: ${actor.offload_model}\n",
    "\n",
    "critic:\n",
    "  model_name: ${actor.model_name}\n",
    "  tokenizer_name: ${critic.model_name}\n",
    "  gradient_checkpointing: ${actor.gradient_checkpointing}\n",
    "  ddp_size: ${actor.ddp_size}\n",
    "  tp_size: ${actor.tp_size}\n",
    "  sp_size: ${actor.sp_size}\n",
    "  max_length_per_device: ${actor.max_length_per_device}\n",
    "  max_inference_length_per_device: ${critic.max_length_per_device}\n",
    "  update_per_rollout: 12\n",
    "  clip: 0.5\n",
    "  agg_mode: ${actor.agg_mode}\n",
    "  lr: 5e-6\n",
    "  weight_decay: ${actor.weight_decay}\n",
    "  max_grad_norm: ${actor.max_grad_norm}\n",
    "  scheduler: ${actor.scheduler}\n",
    "  warmup_ratio: ${actor.warmup_ratio}\n",
    "  offload_model: ${actor.offload_model}\n",
    "  offload_optimizer: ${actor.offload_optimizer}\n",
    "\n",
    "adv:\n",
    "  estimator: reinforce # `reinforce` or `gae`\n",
    "  gamma: 1.0\n",
    "  lamda: 1.0\n",
    "  global_norm: false\n",
    "  norm_var: false\n",
    "  \n",
    "trainer:\n",
    "  project: null\n",
    "  experiment_name: null\n",
    "  load_ckpt_from: null\n",
    "  n_epochs: 1\n",
    "  test_freq: null\n",
    "  save_dir: ckpts/${trainer.experiment_name}\n",
    "  save_freq: null\n",
    "  use_wandb: false\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
